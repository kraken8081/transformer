{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: 起始标记\n",
    "# E: 结束标记\n",
    "# P: padding，将当前序列补齐至最长序列长度的占位符\n",
    "sentence = [\n",
    "    # enc_input dec_input dec_output\n",
    "    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n",
    "]\n",
    "\n",
    "# 词典，padding用0来表示\n",
    "\n",
    "# 源词典\n",
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}\n",
    "src_vocab_size = len(src_vocab) # 6\n",
    "# 目标词典（包含特殊符）\n",
    "tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n",
    "# 反向映射词典，idx --> word\n",
    "idx2word = {v: k for k, v in tgt_vocab.items()}\n",
    "'''\n",
    "{0: 'P',\n",
    " 1: 'i',\n",
    " 2: 'want',\n",
    " 3: 'a',\n",
    " 4: 'beer',\n",
    " 5: 'coke',\n",
    " 6: 'S',\n",
    " 7: 'E',\n",
    " 8: '.'}\n",
    "'''\n",
    "tgt_vocab_size = len(tgt_vocab) # 9\n",
    "\n",
    "src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n",
    "tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
       " ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " 'i': 1,\n",
       " 'want': 2,\n",
       " 'a': 3,\n",
       " 'beer': 4,\n",
       " 'coke': 5,\n",
       " 'S': 6,\n",
       " 'E': 7,\n",
       " '.': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enc_inputs: \n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 5, 0]])\n",
      " dec_inputs: \n",
      " tensor([[6, 1, 2, 3, 4, 8],\n",
      "        [6, 1, 2, 3, 5, 8]])\n",
      " dec_outputs: \n",
      " tensor([[1, 2, 3, 4, 8, 7],\n",
      "        [1, 2, 3, 5, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 这个函数把原始输入序列转换成token表示\n",
    "def make_data(sentence):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentence)):\n",
    "        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n",
    "        \n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "    \n",
    "    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n",
    "\n",
    "print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n",
    "print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n",
    "print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset加载数据\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        # enc_inputs.shape = [2, 5], 返回的是2\n",
    "        return self.enc_inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "loader = torch.utils.data.DataLoader(dataset=MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个词的向量长度\n",
    "d_model = 512\n",
    "\n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    "\n",
    "# 多头注意力后的q, k, v词向量长度，这里是512/8=64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    "\n",
    "# Encoder Layer 和 Decoder Layer 的个数\n",
    "n_layers = 6\n",
    "\n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer包含Encoder和Decoder\n",
    "- Encoder和Decoder各自包含6个Layer\n",
    "- Encoder Layer中包含 Self Attention 和 FFN 两个Sub Layer\n",
    "- Decoder Layer中包含 Masked Self Attention、 Cross Attention、 FFN 三个Sub Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Positional Encoding\n",
    "\n",
    "> 用于为输入的词向量进行位置编码\n",
    "\n",
    "原文：The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posedet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
