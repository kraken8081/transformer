{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S: 起始标记\n",
    "# E: 结束标记\n",
    "# P: padding，将当前序列补齐至最长序列长度的占位符\n",
    "sentence = [\n",
    "    # enc_input dec_input dec_output\n",
    "    ['ich mochte ein bier P','S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P','S i want a coke .', 'i want a coke . E'],\n",
    "]\n",
    "\n",
    "# 词典，padding用0来表示\n",
    "\n",
    "# 源词典\n",
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}\n",
    "src_vocab_size = len(src_vocab) # 6\n",
    "# 目标词典（包含特殊符）\n",
    "tgt_vocab = {'P':0,'i':1,'want':2,'a':3,'beer':4,'coke':5,'S':6,'E':7,'.':8}\n",
    "# 反向映射词典，idx --> word\n",
    "idx2word = {v: k for k, v in tgt_vocab.items()}\n",
    "'''\n",
    "{0: 'P',\n",
    " 1: 'i',\n",
    " 2: 'want',\n",
    " 3: 'a',\n",
    " 4: 'beer',\n",
    " 5: 'coke',\n",
    " 6: 'S',\n",
    " 7: 'E',\n",
    " 8: '.'}\n",
    "'''\n",
    "tgt_vocab_size = len(tgt_vocab) # 9\n",
    "\n",
    "src_len = 5 # 输入序列enc_input的最长序列长度，其实就是最长的那句话的token数\n",
    "tgt_len = 6 # 输出序列dec_input/dec_output的最长序列长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
       " ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " 'i': 1,\n",
       " 'want': 2,\n",
       " 'a': 3,\n",
       " 'beer': 4,\n",
       " 'coke': 5,\n",
       " 'S': 6,\n",
       " 'E': 7,\n",
       " '.': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enc_inputs: \n",
      " tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 5, 0]])\n",
      " dec_inputs: \n",
      " tensor([[6, 1, 2, 3, 4, 8],\n",
      "        [6, 1, 2, 3, 5, 8]])\n",
      " dec_outputs: \n",
      " tensor([[1, 2, 3, 4, 8, 7],\n",
      "        [1, 2, 3, 5, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "# 这个函数把原始输入序列转换成token表示\n",
    "def make_data(sentence):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentence)):\n",
    "        enc_input = [src_vocab[word] for word in sentence[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentence[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentence[i][2].split()]\n",
    "        \n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "    \n",
    "    # LongTensor是专用于存储整型的，Tensor则可以存浮点、整数、bool等多种类型\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentence)\n",
    "\n",
    "print(' enc_inputs: \\n', enc_inputs)  # enc_inputs: [2,5]\n",
    "print(' dec_inputs: \\n', dec_inputs)  # dec_inputs: [2,6]\n",
    "print(' dec_outputs: \\n', dec_outputs) # dec_outputs: [2,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Dataset加载数据\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(MyDataSet, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "        \n",
    "    def __len__(self):\n",
    "        # enc_inputs.shape = [2, 5], 返回的是2\n",
    "        return self.enc_inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "loader = torch.utils.data.DataLoader(dataset=MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个词的向量长度\n",
    "d_model = 512\n",
    "\n",
    "# FFN的隐藏层神经元个数\n",
    "d_ff = 2048\n",
    "\n",
    "# 多头注意力后的q, k, v词向量长度，这里是512/8=64\n",
    "# 原文：queries and kes of dimention d_k,and values of dimension d_v .所以q和k的长度都用d_k来表示\n",
    "d_k = d_v = 64\n",
    "\n",
    "# Encoder Layer 和 Decoder Layer 的个数\n",
    "n_layers = 6\n",
    "\n",
    "# 多头注意力中head的个数，原文：we employ h = 8 parallel attention layers, or heads\n",
    "n_heads = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer包含Encoder和Decoder\n",
    "- Encoder和Decoder各自包含6个Layer\n",
    "- Encoder Layer中包含 Self Attention 和 FFN 两个Sub Layer\n",
    "- Decoder Layer中包含 Masked Self Attention、 Cross Attention、 FFN 三个Sub Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Positional Encoding\n",
    "\n",
    "> 用于为输入的词向量进行位置编码\n",
    "\n",
    "原文：The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        # 开始位置编码部分,先生成一个max_len * d_model 的矩阵，即5000 * 512\n",
    "        # 5000是一个句子中最多的token数，512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # pos：[max_len,1],即[5000,1]\n",
    "        div_term = pos / pow(10000.0, torch.arange(0, d_model, 2).float() / d_model) # div_term：[5000,256]\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(div_term)\n",
    "        pe[:, 1::2] = torch.cos(div_term)\n",
    "        # 一个句子要做一次pe，一个batch中会有多个句子，所以增加一维用来和输入的一个batch的数据相加时做广播\n",
    "        pe = pe.unsqueeze(0) # [5000,512] -> [1,5000,512] \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''x: [batch_size, seq_len, d_model]'''\n",
    "        # 5000是我们预定义的最大的seq_len，就是说我们把最多的情况pe都算好了，用的时候用多少就取多少\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 4.6416e-02, 2.1544e-03],\n",
      "        [2.0000e+00, 9.2832e-02, 4.3089e-03],\n",
      "        [3.0000e+00, 1.3925e-01, 6.4633e-03],\n",
      "        [4.0000e+00, 1.8566e-01, 8.6177e-03],\n",
      "        [5.0000e+00, 2.3208e-01, 1.0772e-02],\n",
      "        [6.0000e+00, 2.7850e-01, 1.2927e-02],\n",
      "        [7.0000e+00, 3.2491e-01, 1.5081e-02],\n",
      "        [8.0000e+00, 3.7133e-01, 1.7235e-02],\n",
      "        [9.0000e+00, 4.1774e-01, 1.9390e-02]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0000,  0.0464,  0.0000,  0.0022,  0.0000],\n",
      "        [ 0.9093,  0.0000,  0.0927,  0.0000,  0.0043,  0.0000],\n",
      "        [ 0.1411,  0.0000,  0.1388,  0.0000,  0.0065,  0.0000],\n",
      "        [-0.7568,  0.0000,  0.1846,  0.0000,  0.0086,  0.0000],\n",
      "        [-0.9589,  0.0000,  0.2300,  0.0000,  0.0108,  0.0000],\n",
      "        [-0.2794,  0.0000,  0.2749,  0.0000,  0.0129,  0.0000],\n",
      "        [ 0.6570,  0.0000,  0.3192,  0.0000,  0.0151,  0.0000],\n",
      "        [ 0.9894,  0.0000,  0.3629,  0.0000,  0.0172,  0.0000],\n",
      "        [ 0.4121,  0.0000,  0.4057,  0.0000,  0.0194,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "pe = torch.zeros(10, 6)\n",
    "\n",
    "pos = torch.arange(0, 10, dtype=torch.float).unsqueeze(1)\n",
    "div_term = pos / pow(10000.0, torch.arange(0, 6, 2).float() / 6) # div_term：[5000,256]\n",
    "print(div_term)\n",
    "pe[:, 0::2] = torch.sin(div_term)\n",
    "# pe[:, 1::2] = torch.cos(div_term)\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(10000.0, torch.arange(0, 6, 2).float() / 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posedet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
